{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram / file 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def fileids(path):\n",
    "    return [path+file for file in listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(term, n=2):\n",
    "    return [term[i:i+n] for i in range(len(term) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "472\n",
      "2510\n",
      "5161\n",
      "1598\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "ma = Komoran()\n",
    "\n",
    "for file in fileids(\"./news_crawl_project/\"):\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "    tokens1 = content.split() # 원시어절\n",
    "    tokens2 = word_tokenize(content) # 구두점 분리 => 원시어절\n",
    "    tokens3 = [_ for token in tokens2 for _ in ma.pos(token)] # 형태소-품사\n",
    "    tokens4 = [token[0] for token in tokens3] # 형태소\n",
    "    tokens5 = [token[0] for token in tokens3 if token[1].startswith('N')] # 명사\n",
    "    tokens6 = [_ for token in tokens4 for _ in ngram(token)] # ngram\n",
    "    \n",
    "    print(\"..\")\n",
    "    print(len(tokens1))\n",
    "    print(len(tokens1 + tokens2 + tokens3))\n",
    "    print(len(tokens1 + tokens2 + tokens3 + tokens4 + tokens5 + tokens6))\n",
    "    print(len(set(tokens1 + tokens2 + tokens3 + tokens4 + tokens5 + tokens6)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filecontent(file):\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "corpus = filecontent(fileids('./news_crawl_project/')[-2])\n",
    "\n",
    "pattern = dict()\n",
    "    \n",
    "# 구두점\n",
    "pattern1 = re.compile(r'[{0}]'.format(re.escape(punctuation)))\n",
    "pattern['punc'] = pattern1\n",
    "# corpus = pattern1.sub(' ',corpus)\n",
    "\n",
    "# 불용어\n",
    "pattern2 = re.compile(r'[A-Za-z0-9]{7,}')\n",
    "pattern['stop'] = pattern2\n",
    "# corpus = pattern2.sub(' ',corpus)\n",
    "\n",
    "# 이메일\n",
    "# pattern3 = re.compile(r'\\w{2,}@\\w{3,}(.\\w{2,})+')\n",
    "pattern3 = re.compile(r'\\w{2,}@(.?\\w{2,})+')\n",
    "pattern['email'] = pattern3\n",
    "# corpus = pattern3.sub(' ',corpus)\n",
    "\n",
    "# 도메인\n",
    "pattern4 = re.compile(r'(.?\\w{2,}){2,}')\n",
    "pattern['url'] = pattern4\n",
    "# corpus = pattern4.sub(' ',corpus)\n",
    "\n",
    "# 한글 이외\n",
    "pattern5 = re.compile(r'[^가-힣0-9]+')\n",
    "pattern['nonkorean'] = pattern5\n",
    "# corpus = pattern5.sub(' ',corpus)\n",
    "\n",
    "# WhiteSpace\n",
    "pattern6 = re.compile(r\"\\s{2,}\")\n",
    "pattern['whitespace'] = pattern5\n",
    "# corpus = pattern6.sub(' ',corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 219, 139, 124, 175, 175, 1, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "content = filecontent(fileids('./news_crawl_project/')[-2])\n",
    "indexTerm1 = defaultdict(int)\n",
    "indexTerm2 = defaultdict(int)\n",
    "indexTerm3 = defaultdict(int)\n",
    "indexTerm4 = defaultdict(int)\n",
    "indexTerm5 = defaultdict(int)\n",
    "indexTerm6 = defaultdict(int)\n",
    "\n",
    "for _ in ['email', 'punc', 'stop','whitespace']:\n",
    "    content = pattern[_].sub(' ',content)\n",
    "\n",
    "for term in word_tokenize(content):\n",
    "    indexTerm1[term] += 1 # 원시어절\n",
    "    \n",
    "for _ in indexTerm1:\n",
    "    for t in ma.pos(_):\n",
    "        indexTerm2[t] += 1 # 원시형태소+품사\n",
    "        if len(t[0]) > 1: # 음절 길이로 정규화\n",
    "            indexTerm3[t[0]] += 1 # 원시형태소\n",
    "        if t[1].startswith('N'):\n",
    "            indexTerm4[t[0]] += 1 # 명사\n",
    "        for n in ngram(t[0]): # 바이그램\n",
    "            indexTerm5[n] += 1\n",
    "            indexTerm6[n] += 1\n",
    "            \n",
    "len(indexTerm1), len(indexTerm2), len(indexTerm3), \\\n",
    "len(indexTerm4), len(indexTerm5), len(indexTerm6), \\\n",
    "min(indexTerm5.values()), max(indexTerm5.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
