{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.10 프로젝트 : 뉴스기사 분야별로 크롤링하여 저장!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "\n",
    "header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36\n",
    "def getDownload(url, params={}, retries=3):\n",
    "    resp = None\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, params=params, headers=header)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500 <= e.response.status_code < 600 and retries > 0:\n",
    "            print(retries)\n",
    "            resp = getDownload(url, params, retries-1)\n",
    "        else:\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤 관련 라이브러리들 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음\n",
    "## 사회, 정치, 경제, 국제, 문화, IT로 나눠서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daum_news_detail_crawl(links):\n",
    "    for link in links:\n",
    "        # 뉴스로만 향하는 링크들만\n",
    "        if link.startswith(\"http://v.media.daum.net/v/\"):\n",
    "\n",
    "            article_id = link.split(\"/\")[-1]\n",
    "\n",
    "            html = getDownload(link)\n",
    "            detail_dom = BeautifulSoup(html.content, \"lxml\")\n",
    "\n",
    "            # 파일 생성\n",
    "            file_name = \"./news_crawl_project/\" + field + \"-\" + article_id\n",
    "            f = open(file_name, 'w', encoding=\"utf-8\")\n",
    "\n",
    "            for _ in detail_dom.select(\"#harmonyContainer > section > p\"):\n",
    "    #           각 p태그 파일에 write\n",
    "                f.write(_.text)\n",
    "\n",
    "            print(file_name, \"생성 완료\")\n",
    "            time.sleep(1)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "url = \"https://media.daum.net/\"\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분야이름 \n",
    "field = browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(2) > a\")[0].text\n",
    "# 사회분야 클릭\n",
    "browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(2) > a\")[0].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 페이지 소스 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = BeautifulSoup(browser.page_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세부링크 가져와서 urljoin -> url방문 -> 내용 긁어오기\n",
    "#### 처음엔 생으로 짜고 이것을 함수로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [_[\"href\"] for _ in dom.select(\".section_cate.section_headline .link_txt\") if _.has_attr('href')]\n",
    "\n",
    "for link in links:\n",
    "    # 뉴스로만 향하는 링크들만\n",
    "    if link.startswith(\"http://v.media.daum.net/v/\"):\n",
    "\n",
    "        article_id = link.split(\"/\")[-1]\n",
    "\n",
    "        html = getDownload(link)\n",
    "        detail_dom = BeautifulSoup(html.content, \"lxml\")\n",
    "        \n",
    "        # 파일 생성\n",
    "        file_name = \"./news_crawl_project/\" + field + \"-\" + article_id\n",
    "        f = open(file_name, 'w', encoding=\"utf-8\")\n",
    "        \n",
    "        for _ in detail_dom.select(\"#harmonyContainer > section > p\"):\n",
    "#           각 p태그 파일에 write\n",
    "            f.write(_.text)\n",
    "        \n",
    "        print(file_name, \"생성 완료\")\n",
    "        \n",
    "        f.close()\n",
    "        #harmonyContainer > section > p:nth-child(3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_elements_by_xpath(\"//*[@id='kakaoGnb']/div/ul/li[3]/a\")[0].text\n",
    "browser.find_elements_by_xpath(\"//*[@id='kakaoGnb']/div/ul/li[3]/a\")[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "links = [_[\"href\"] for _ in dom.select(\".section_cate.section_headline .link_txt\") if _.has_attr('href')]\n",
    "\n",
    "daum_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(4) > a\")[0].text\n",
    "browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(4) > a\")[0].click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\".section_cate.section_headline .link_txt\") if _.has_attr('href')]\n",
    "\n",
    "daum_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(5) > a\")[0].text\n",
    "browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(5) > a\")[0].click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\".section_cate.section_headline .link_txt\") if _.has_attr('href')]\n",
    "\n",
    "daum_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(6) > a\")[0].text\n",
    "browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(6) > a\")[0].click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\".section_cate.section_headline .link_txt\") if _.has_attr('href')]\n",
    "\n",
    "daum_news_detail_crawl(links)\n",
    "\n",
    "        #harmonyContainer > section > p:nth-child(3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(7) > a\")[0].text\n",
    "browser.find_elements_by_css_selector(\"#kakaoGnb > div > ul > li:nth-child(7) > a\")[0].click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "daum_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이트 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "url = \"https://news.nate.com\"\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nate_news_detail_crawl(links):\n",
    "    print(field, \"분야 개수 : \", len(links))\n",
    "    for link in links:\n",
    "        if link.startswith(\"//news.nate.com/view/\"):\n",
    "            article_id = link.split(\"/\")[-1].split('?')[0]\n",
    "\n",
    "            html = getDownload(\"https:\" + link)\n",
    "            detail_dom = BeautifulSoup(html.content, \"lxml\")\n",
    "\n",
    "            # 파일 생성\n",
    "            file_name = \"./news_crawl_project/\" + field + \"-\" + article_id\n",
    "            f = open(file_name, 'w', encoding=\"utf-8\")\n",
    "\n",
    "            for _ in detail_dom.select(\"#realArtcContents\"):\n",
    "    #           각 p태그 파일에 write\n",
    "                f.write(_.text)\n",
    "\n",
    "            print(file_name, \"생성 완료\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_element_by_class_name(\"news03\").text\n",
    "browser.find_element_by_class_name(\"news03\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\"#newsContents a\") if _.has_attr('href')]\n",
    "\n",
    "nate_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_element_by_class_name(\"news04\").text\n",
    "browser.find_element_by_class_name(\"news04\").click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\"#newsContents a\") if _.has_attr('href')]\n",
    "\n",
    "nate_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_element_by_class_name(\"news05\").text\n",
    "browser.find_element_by_class_name(\"news05\").click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\"#newsContents a\") if _.has_attr('href')]\n",
    "\n",
    "nate_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_element_by_class_name(\"news06\").text\n",
    "browser.find_element_by_class_name(\"news06\").click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\"#newsContents a\") if _.has_attr('href')]\n",
    "\n",
    "nate_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT/과학"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = browser.find_element_by_class_name(\"news07\").text[:2]\n",
    "browser.find_element_by_class_name(\"news07\").click()\n",
    "\n",
    "dom = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "links = [_[\"href\"] for _ in dom.select(\"#newsContents a\") if _.has_attr('href')]\n",
    "\n",
    "nate_news_detail_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외 : 네이버 뉴스\n",
    "## 이번엔 beautifulsoup만 써서 많이 본 뉴스 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/\"\n",
    "html = getDownload(url)\n",
    "dom = BeautifulSoup(html.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필드 리스트 따로뽑고 10개씩 짤라서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = dom.select(\".category_ranking\")[0].text.split(\" \")\n",
    "field_list = list(filter(None, field_list))\n",
    "len(field_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랭크가 10개씩 있으니 10개될때마다 1로 리셋하면서 긁어오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select(\".section_list_ranking a\")[-1][\"href\"].split(\"=\")[5].split(\"&\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1\n",
    "index = 0\n",
    "field = field_list[index]\n",
    "links = dom.select(\".section_list_ranking a\")\n",
    "\n",
    "for _ in links:\n",
    "    \n",
    "    link = requests.compat.urljoin(url, _[\"href\"])\n",
    "    html = getDownload(link)\n",
    "    detail_dom = BeautifulSoup(html.content, \"lxml\")\n",
    "    \n",
    "    # /로 필드이름이 있으면 디렉토리로 인식해서 예외문을 만들었다.\n",
    "    try:\n",
    "        file_name = \"./news_crawl_project/\" + field + \"-naver\" + _[\"href\"].split(\"=\")[5].split(\"&\")[0]\n",
    "        f = open(file_name, 'w', encoding=\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        file_name = \"./news_crawl_project/\" + field.split(\"/\")[0] + \"-naver\" + _[\"href\"].split(\"=\")[5].split(\"&\")[0]\n",
    "        f = open(file_name, 'w', encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "    for _ in detail_dom.select(\"#articleBodyContents\"):\n",
    "        f.write(_.text)\n",
    "\n",
    "    print(file_name, \"생성 완료\")\n",
    "#     time.sleep(1)\n",
    "   \n",
    "    rank += 1\n",
    "    \n",
    "    # rank랑 field 리셋\n",
    "    if rank > 10 :\n",
    "        index += 1\n",
    "        rank = 1\n",
    "        if index < len(field_list):\n",
    "            field = field_list[index]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
