{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.16 Normalization\n",
    "## 1. 대소문자 통합(소문자)\n",
    "## 2. 구두점 처리 (I`d, I`m) => 기본적으로는 분리 tokenize\n",
    "##   => 형태소분석기가 기본적으로 해줌\n",
    "## 3. 불용어(stopwords) 처리\n",
    "\n",
    "## TOKENIZATION => 영어(lowercase) => 구두점(in, re) => 불용어(stopwords, [사전] in koreand => 단어의 길이, 빈도 => filtering = 욕설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I`d like to learn more something\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구두점 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation \n",
    "# Finland`s -> finland. finlands 어떤식으로 변형시킬지\n",
    "# '오늘'의 => 오늘 의, 오늘의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.escape(punctuation) # 정규식표현으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id like to learn more something'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[{0}]\".format(re.escape(punctuation)), \"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[\\!\\\"\\#\\$\\%\\&\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"[{0}]\".format(re.escape(punctuation)))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id like to learn more something\n",
      "i ` d like to learn more something\n",
      "`차이를 알 수 있다\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(pattern.sub(\"\", sentence.lower()))\n",
    "print(\" \".join(word_tokenize(sentence.lower())))\n",
    "print(\"`차이를 알 수 있다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘은 목요일\n",
      "오늘은 '목'요일\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘은 '목'요일\"\n",
    "print(pattern.sub(\"\", sentence.lower()))\n",
    "print(\" \".join(word_tokenize(sentence.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i d asdf 2342 '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\\s{2,}\", \" \", \"i  d  asdf  2342 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "#### 어근이 아닌애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(936, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop = stopwords.open(\"english\").read()\n",
    "len(stop), print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n",
      "Skipped\n",
      "better\n",
      "Skipped\n",
      "ugly\n",
      "Skipped\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love you\"\n",
    "sentence = \"Beautiful is better than ugly.\"\n",
    "# for _ in sentence.lower().split():\n",
    "#     if _ in stop:\n",
    "#         print(\"Skipped\")\n",
    "#     else:\n",
    "#         print(_)\n",
    "# 한글은 1음절단위라 stopwords는 ngram으로 처리해야한다\n",
    "for _ in word_tokenize(sentence.lower()): # 1번\n",
    "    if pattern.sub(\"\", _) in stop: # 2번 with stopwords\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords 처리하면 단어의수가 엄청 줄어든다. 19만개 -> 6만개\n",
    "### 하지만 중요한 말이 포함될 수 도 있다. ex) \"to be or not to be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "corpus = gutenberg.open(\"austen-emma.txt\").read()\n",
    "\n",
    "len(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69791"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list()\n",
    "for _ in word_tokenize(corpus.lower()):\n",
    "    if pattern.sub(\"\", _) not in stop:\n",
    "        words.append(_)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어머니', '는', '짜장면', '이', '싫다', '고', '하셨', '어', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korstop =[\"은\", \"는\", \"이\", \"기\", \"을\", \"를\", \"께서\", \"에게\", \"을\", \"를\", \"고\", \"어\", \".\"]\n",
    "\n",
    "sentence = \"어머니 는 짜장면 이 싫다 고 하셨 어.\"\n",
    "\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어머니', '짜장면', '싫다', '하셨']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in word_tokenize(sentence) if _ not in korstop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 길이 정규화 => 특정 길이(너무 짧거나 너무 길거나)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "learn\n",
      "more\n",
      "like\n",
      "learn\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more something. I'd like to learn. I'd like\"\n",
    "for _ in pattern.sub(\"\", sentence.lower()).split():\n",
    "    if 2 < len(_) < 6:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'to', 'learn', 'more', 'like', 'to', 'learn', 'like']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum = 2\n",
    "maximum = 6\n",
    "pattern2 = re.compile(r\"\\b\\w{%d,%d}\\b\" % (minimum, maximum))\n",
    "pattern2.findall(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to 2\n",
      "learn 2\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "obj = Text(word_tokenize(pattern.sub(\"\",sentence.lower())))\n",
    "for _ in obj.vocab():\n",
    "    if 1 < obj.vocab().get(_) < 3:\n",
    "        print(_, obj.vocab().get(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치로 정규화 해보기\n",
    "### N : 전체단어개수\n",
    "### B : 유니크한 단어 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191785 8406 [(',', 12016), ('.', 6355), ('to', 5125), ('the', 4844), ('and', 4653), ('of', 4272), ('I', 3177), ('--', 3100), ('a', 3001), (\"''\", 2452), ('was', 2383), ('her', 2360), (';', 2353), ('not', 2242), ('in', 2103), ('it', 2103), ('be', 1965), ('she', 1774), ('``', 1735), ('that', 1729), ('you', 1664), ('had', 1605), ('as', 1387), ('he', 1365), ('for', 1320), ('have', 1301), ('is', 1221), ('with', 1185), ('very', 1151), ('but', 1148), ('Mr.', 1091), ('his', 1084), ('!', 1063), ('at', 996), ('so', 918), (\"'s\", 866), ('Emma', 855), ('all', 831), ('could', 824), ('would', 813), ('been', 755), ('him', 748), ('on', 674), ('Mrs.', 668), ('any', 651), ('?', 621), ('my', 619), ('no', 616), ('Miss', 592), ('were', 590)]\n",
      "191781 7944\n",
      "158270 9311\n",
      "162122 7102\n",
      "70168 6933\n"
     ]
    }
   ],
   "source": [
    "original = Text(word_tokenize(corpus))\n",
    "print(original.vocab().N(), original.vocab().B(), original.vocab().most_common(50))\n",
    "\n",
    "lowercase = Text(word_tokenize(corpus.lower()))\n",
    "print(lowercase.vocab().N(), lowercase.vocab().B())\n",
    "\n",
    "punct1 = Text(word_tokenize(pattern.sub(\"\", corpus.lower())))\n",
    "print(punct1.vocab().N(), punct1.vocab().B())\n",
    "\n",
    "punct2 = Text(word_tokenize(pattern.sub(\" \", corpus.lower())))\n",
    "print(punct2.vocab().N(), punct2.vocab().B())\n",
    "\n",
    "stops = Text([_ for _ in word_tokenize(pattern.sub(\" \", corpus.lower())) if _ not in stop])\n",
    "print(stops.vocab().N(), stops.vocab().B())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 글자길이 > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63814 6763\n"
     ]
    }
   ],
   "source": [
    "advanced = Text([_ for _ in word_tokenize(pattern.sub(\" \", corpus.lower())) if _ not in stop and len(_) > 3])\n",
    "\n",
    "print(advanced.vocab().N(), advanced.vocab().B())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63814 6763\n"
     ]
    }
   ],
   "source": [
    "advanced = Text([_ for _ in word_tokenize(pattern.sub(\" \", corpus.lower())) if _ not in stop and \\\n",
    "                 re.search(r\"\\b\\w{4,}\\b\", _)])\n",
    "print(advanced.vocab().N(), advanced.vocab().B())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48961 1111\n"
     ]
    }
   ],
   "source": [
    "advanced = Text([_ for _ in word_tokenize(pattern.sub(\" \", corpus.lower())) if _ not in stop and len(_) > 3])\n",
    "result = list()\n",
    "for _ in advanced:\n",
    "    if advanced.vocab().get(_) > 10:\n",
    "        result.append(_)\n",
    "        \n",
    "print(Text(result).vocab().N(), Text(result).vocab().B())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48961 1111\n"
     ]
    }
   ],
   "source": [
    "freq = [(_, advanced.count(_)) for _ in advanced.vocab()\n",
    "            if advanced.vocab().get(_) > 10]\n",
    "print(sum([_[1] for _ in freq]), len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01766712281203407"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Text(result).vocab())\n",
    "Text(result).vocab().freq(\"emma\") # \"emma\"가 차지하는 부분을 알려줌 바율로 자르는게 빈도수로 자르는것보다 안정적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 불용어와 비속어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** ***** ************* 시1발 시!발'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# => 사전정보 lexicon resource (못구함 돈이기때문에)\n",
    "stop = [\"시발\"]\n",
    "sentence = \"시발 시발짜증나 시발!ㅁㄴㅇㅇㄻㄴㅇ234 시1발 시!발\"\n",
    "\n",
    "result = list()\n",
    "for _ in sentence.split():\n",
    "#     if _ not in stop:\n",
    "    if not re.search(stop[0], _):\n",
    "        result.append(_)\n",
    "    else:\n",
    "        result.append(\"*\" * len(_))\n",
    "\n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram을 사용해서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram 구현\n",
    "def umjeol(text, n=2):\n",
    "    ngram = list()\n",
    "    for i in range(len(text)-(n-1)):\n",
    "        ngram.append(\"\".join(text[i:i+n]))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** ***** ************* 시1발 시!발'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list()\n",
    "for _ in sentence.split():\n",
    "    flag = False\n",
    "    for ngram in umjeol(_):\n",
    "        if ngram in stop:\n",
    "            flag = True\n",
    "\n",
    "    if not flag:\n",
    "        result.append(_)\n",
    "    else:\n",
    "        result.append(\"*\" * len(_))\n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** ***** ************* *** 시!발'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"시발 시발짜증나 시발!ㅁㄴㅇㅇㄻㄴㅇ234 시1발 시!발\"\n",
    "\n",
    "result = list()\n",
    "for _ in sentence.split():\n",
    "    if not re.search(stop[0], re.sub(r\"\\B[0-9!]+\\B\", \"\", _)):\n",
    "        result.append(_)\n",
    "    else:\n",
    "        result.append(\"*\" * len(_))\n",
    "        \n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte pair encoding\n",
    "#### stopwords => list(dictionary) 힘들가\n",
    "### BPE => 패턴(쌍)찾아서 적용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTerm(term):\n",
    "    result = list()\n",
    "    for token in term.split():\n",
    "        result.append(\" \".join([\"</w>\"] + list(term) + [\"</w>\"]))\n",
    "    return \" _ \".join(result)\n",
    "#     print(\" \".join(list(term) + [\"</w>\"])) # /w는 단어의 경계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mergeNgram(maxKey, data):\n",
    "    newData = dict()\n",
    "    \n",
    "    for term, freq in data.items():\n",
    "        newKey = re.sub(maxKey, maxKey.replace(' ',''), term)\n",
    "        newData[newKey] = freq\n",
    "    \n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram(data, n=2):\n",
    "    result = defaultdict(int)\n",
    "    for term, freq in data.items():\n",
    "        tokens = term.split()\n",
    "        for i in range(len(tokens)-(n-1)):\n",
    "#             key = \" \".join(tokens[i:i+n])\n",
    "#             if key in result.keys():\n",
    "#                 result[key] += freq\n",
    "#             else:\n",
    "#                 result[key] = freq\n",
    "            result[\" \".join(tokens[i:i+n])] += freq\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    splitTerm(\"시발\"):2,\n",
    "    splitTerm(\"시1발\"):2,\n",
    "    splitTerm(\"시~@1발\"):6,\n",
    "    splitTerm(\"시123123135발\"):3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'</w> 시': 13,\n",
       "             '시 발': 2,\n",
       "             '발 </w>': 13,\n",
       "             '시 1': 5,\n",
       "             '1 발': 8,\n",
       "             '시 ~': 6,\n",
       "             '~ @': 6,\n",
       "             '@ 1': 6,\n",
       "             '1 2': 6,\n",
       "             '2 3': 6,\n",
       "             '3 1': 6,\n",
       "             '1 3': 3,\n",
       "             '3 5': 3,\n",
       "             '5 발': 3})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = ngram(data)\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</w>시 발</w>': 2,\n",
       " '</w>시 1 발</w>': 2,\n",
       " '</w>시 ~ @ 1 발</w>': 6,\n",
       " '</w>시 1 2 3 1 2 3 1 3 5 발</w>': 3}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    bigram = ngram(data)\n",
    "    maxKey = max(bigram, key=bigram.get)\n",
    "    data = mergeNgram(maxKey, data)\n",
    "#     print(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('발</w>', 13),\n",
       " ('</w>시', 13),\n",
       " ('1', 11),\n",
       " ('@', 6),\n",
       " ('~', 6),\n",
       " ('2', 3),\n",
       " ('3', 3),\n",
       " ('5', 3)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = defaultdict(int)\n",
    "for _ in data:\n",
    "    for token in set(_.split()):\n",
    "        pattern[token] += data[_]\n",
    "\n",
    "sorted(pattern.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-c147b294908b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopRE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"{0}.*{1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "stopRE = re.compile(r\"{0}.*{1}\".format(pattern[0][0], pattern[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in [\"</w>\" + _ + \"</w>\" for _ in sentence.split()]:\n",
    "    if stopRE.search(_):\n",
    "        print(_)\n",
    "    else:\n",
    "        print(\"Skipped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-fd01606bd375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmaxKey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmergeNgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxKey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-1fd1ae16a446>\u001b[0m in \u001b[0;36mngram\u001b[1;34m(data, n)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#             else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#                 result[key] = freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    bigram = ngram(data)\n",
    "    maxKey = max(bigram, key=bigram.get)\n",
    "    data = mergeNgram(maxKey, data)\n",
    "#     print(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 형태소 분석기 테스트 feat. Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, singular, common\n",
      "    failure burden court fire appointment awarding compensation Mayor\n",
      "    interim committee fact effect airport management surveillance jail\n",
      "    doctor intern extern night weekend duty legislation Tax Office ...\n",
      "NN$: noun, singular, common, genitive\n",
      "    season's world's player's night's chapter's golf's football's\n",
      "    baseball's club's U.'s coach's bride's bridegroom's board's county's\n",
      "    firm's company's superintendent's mob's Navy's ...\n",
      "NN+BEZ: noun, singular, common + verb 'to be', present tense, 3rd person singular\n",
      "    water's camera's sky's kid's Pa's heat's throat's father's money's\n",
      "    undersecretary's granite's level's wife's fat's Knife's fire's name's\n",
      "    hell's leg's sun's roulette's cane's guy's kind's baseball's ...\n",
      "NN+HVD: noun, singular, common + verb 'to have', past tense\n",
      "    Pa'd\n",
      "NN+HVZ: noun, singular, common + verb 'to have', present tense, 3rd person singular\n",
      "    guy's Knife's boat's summer's rain's company's\n",
      "NN+IN: noun, singular, common + preposition\n",
      "    buncha\n",
      "NN+MD: noun, singular, common + modal auxillary\n",
      "    cowhand'd sun'll\n",
      "NN+NN: noun, singular, common, hyphenated pair\n",
      "    stomach-belly\n",
      "NNS: noun, plural, common\n",
      "    irregularities presentments thanks reports voters laws legislators\n",
      "    years areas adjustments chambers $100 bonds courts sales details raises\n",
      "    sessions members congressmen votes polls calls ...\n",
      "NNS$: noun, plural, common, genitive\n",
      "    taxpayers' children's members' States' women's cutters' motorists'\n",
      "    steelmakers' hours' Nations' lawyers' prisoners' architects' tourists'\n",
      "    Employers' secretaries' Rogues' ...\n",
      "NNS+MD: noun, plural, common + modal auxillary\n",
      "    duds'd oystchers'll\n",
      "NP: noun, singular, proper\n",
      "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
      "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
      "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
      "NP$: noun, singular, proper, genitive\n",
      "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
      "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
      "    Shea's Palmer's Arnold's Broglio's ...\n",
      "NP+BEZ: noun, singular, proper + verb 'to be', present tense, 3rd person singular\n",
      "    W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's Seaton's\n",
      "    Buckhorn's Breed's Penny's Rob's Kitty's Blackwell's Myra's Wally's\n",
      "    Lucille's Springfield's Arlene's\n",
      "NP+HVZ: noun, singular, proper + verb 'to have', present tense, 3rd person singular\n",
      "    Bill's Guardino's Celie's Skolman's Crosson's Tim's Wally's\n",
      "NP+MD: noun, singular, proper + modal auxillary\n",
      "    Gyp'll John'll\n",
      "NPS: noun, plural, proper\n",
      "    Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen\n",
      "    Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British\n",
      "    Dixiecrats Mississippians Congresses ...\n",
      "NPS$: noun, plural, proper, genitive\n",
      "    Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees' Stevenses'\n",
      "    Geraghtys' Burkes' Wackers' Achaeans' Dresbachs' Russians' Democrats'\n",
      "    Gershwins' Adventists' Negroes' Catholics' ...\n",
      "NR: noun, singular, adverbial\n",
      "    Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow\n",
      "    tonight West East Saturday west left east downtown north northeast\n",
      "    southeast northwest North South right ...\n",
      "NR$: noun, singular, adverbial, genitive\n",
      "    Saturday's Monday's yesterday's tonight's tomorrow's Sunday's\n",
      "    Wednesday's Friday's today's Tuesday's West's Today's South's\n",
      "NR+MD: noun, singular, adverbial + modal auxillary\n",
      "    today'll\n",
      "NRS: noun, plural, adverbial\n",
      "    Sundays Mondays Saturdays Wednesdays Souths Fridays\n"
     ]
    }
   ],
   "source": [
    "from nltk.help import brown_tagset\n",
    "\n",
    "brown_tagset(\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanford  tagger가 제일 유명함 model을 넣어야돼서 따로 다운로드 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "MODEL = \"./stanford-postagger-full-2018-10-16/models/english-bidirectional-distsim.tagger\"\n",
    "PARSER = \"./stanford-postagger-full-2018-10-16/stanford-postagger-3.9.2.jar\"\n",
    "pos = StanfordPOSTagger(MODEL, PARSER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NNP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('I', 'NNP'),\n",
       " ('CHAPTER', 'NNP'),\n",
       " ('I', 'NNP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " (',', ','),\n",
       " ('handsome', 'JJ'),\n",
       " (',', ','),\n",
       " ('clever', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('rich', 'JJ'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('comfortable', 'JJ'),\n",
       " ('home', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('happy', 'JJ'),\n",
       " ('disposition', 'NN'),\n",
       " (',', ','),\n",
       " ('seemed', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('unite', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('blessings', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('existence', 'NN'),\n",
       " (';', ':'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " ('lived', 'VBN'),\n",
       " ('nearly', 'RB'),\n",
       " ('twenty-one', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('very', 'RB'),\n",
       " ('little', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('distress', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('vex', 'VB'),\n",
       " ('her', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "pos.tag(word_tokenize(sent_tokenize(corpus)[0]))\n",
    "# pos.tag_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EC': '연결 어미',\n",
       " 'ECD': '의존적 연결 어미',\n",
       " 'ECE': '대등 연결 어미',\n",
       " 'ECS': '보조적 연결 어미',\n",
       " 'EF': '종결 어미',\n",
       " 'EFA': '청유형 종결 어미',\n",
       " 'EFI': '감탄형 종결 어미',\n",
       " 'EFN': '평서형 종결 어미',\n",
       " 'EFO': '명령형 종결 어미',\n",
       " 'EFQ': '의문형 종결 어미',\n",
       " 'EFR': '존칭형 종결 어미',\n",
       " 'EP': '선어말 어미',\n",
       " 'EPH': '존칭 선어말 어미',\n",
       " 'EPP': '공손 선어말 어미',\n",
       " 'EPT': '시제 선어말 어미',\n",
       " 'ET': '전성 어미',\n",
       " 'ETD': '관형형 전성 어미',\n",
       " 'ETN': '명사형 전성 어미',\n",
       " 'IC': '감탄사',\n",
       " 'JC': '접속 조사',\n",
       " 'JK': '조사',\n",
       " 'JKC': '보격 조사',\n",
       " 'JKG': '관형격 조사',\n",
       " 'JKI': '호격 조사',\n",
       " 'JKM': '부사격 조사',\n",
       " 'JKO': '목적격 조사',\n",
       " 'JKQ': '인용격 조사',\n",
       " 'JKS': '주격 조사',\n",
       " 'JX': '보조사',\n",
       " 'MA': '부사',\n",
       " 'MAC': '접속 부사',\n",
       " 'MAG': '일반 부사',\n",
       " 'MD': '관형사',\n",
       " 'MDN': '수 관형사',\n",
       " 'MDT': '일반 관형사',\n",
       " 'NN': '명사',\n",
       " 'NNB': '일반 의존 명사',\n",
       " 'NNG': '보통명사',\n",
       " 'NNM': '단위 의존 명사',\n",
       " 'NNP': '고유명사',\n",
       " 'NP': '대명사',\n",
       " 'NR': '수사',\n",
       " 'OH': '한자',\n",
       " 'OL': '외국어',\n",
       " 'ON': '숫자',\n",
       " 'SE': '줄임표',\n",
       " 'SF': '마침표, 물음표, 느낌표',\n",
       " 'SO': '붙임표(물결,숨김,빠짐)',\n",
       " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
       " 'SS': '따옴표,괄호표,줄표',\n",
       " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
       " 'UN': '명사추정범주',\n",
       " 'VA': '형용사',\n",
       " 'VC': '지정사',\n",
       " 'VCN': \"부정 지정사, 형용사 '아니다'\",\n",
       " 'VCP': \"긍정 지정사, 서술격 조사 '이다'\",\n",
       " 'VV': '동사',\n",
       " 'VX': '보조 용언',\n",
       " 'VXA': '보조 형용사',\n",
       " 'VXV': '보조 동사',\n",
       " 'XP': '접두사',\n",
       " 'XPN': '체언 접두사',\n",
       " 'XPV': '용언 접두사',\n",
       " 'XR': '어근',\n",
       " 'XSA': '형용사 파생 접미사',\n",
       " 'XSN': '명사파생 접미사',\n",
       " 'XSV': '동사 파생 접미사'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "Kkma().tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
