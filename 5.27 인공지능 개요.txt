인공지능 역사

오버피팅 언더피팅
머신러닝은 적정선에서 끝낼 줄 알아야 한다.
VALIDAITION CHECK를 해야한다. (테스트 셋 따로, CROSS VALIDATION)



머신러닝 기초수학
자연로그 현실을 실제로 잘 표현하는 함수이다.
공간은 벡터를 담고 있고 너무 많이 담고 있다. 하지만 이것을 다 담을 필요는 없다. CNN에서 활용 UNIT VECTOR

y = f(x) x:독립변수, y:종속변수

베이지안 확률이 계속 쓰이고 있다.



예측문제
회귀 : 션형회귀 vs 다중선형회귀 - 독립변수 개수 차이
분류 : knn 단순하고 쉽게 비선형 알고리즘을 구현할 수 있지만 k를 직접설정해야하는 문제
       svm : hyperlane을 쓰면 n-1차원 된다
       decision tree 
       k-means는 명확하게 군집수가 정해져 있으면 좋다.
       dbscan 밀도있게 연결돼 있는 데이터 집합은 동일한 cluster - parameter정하기가 힘들다
       hierarchical 단계별로 특징들을 파악하며 군집화할 수 있다.



강화학습
다이나믹 프로그래밍
markov chain - 차원의 저주  - 동적프로그래밍, 몬테카를로, 시간차 방법이 등장
딥러닝 역사
Drop Out 알고리즘(2012) : 학습 시 오버피팅을 줄임 - 몇개 버리는거
sigmoid형 함수 대신 ReLU형 함수 사용 - 0에 가까운 수는 반영 x


딥러닝의 수학적 원리
xor
활성화 함수, softmax함수, 
경사하강법
오류역전파


CNN - pooling layer - 이미지 처리
RNN - 가변적길이 데이터 - 자연어 음성어 처리
