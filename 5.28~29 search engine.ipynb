{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram / file 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def fileids(path):\n",
    "    return [path+file for file in listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(term, n=2):\n",
    "    return [term[i:i+n] for i in range(len(term) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "ma = Komoran()\n",
    "\n",
    "# 여러가지 토큰 방법 일단은\n",
    "def tokenize(file):\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "    tokens1 = content.split() # 원시어절\n",
    "    tokens2 = word_tokenize(content) # 구두점 분리 => 원시어절\n",
    "    tokens3 = [_ for token in tokens2 for _ in ma.pos(token)] # 형태소-품사\n",
    "    tokens4 = [token[0] for token in tokens3] # 형태소\n",
    "    tokens5 = [token[0] for token in tokens3 if token[1].startswith('N')] # 명사\n",
    "    tokens6 = [_ for token in tokens4 for _ in ngram(token)] # ngram\n",
    "\n",
    "    print(\"..\")\n",
    "    print(len(tokens1))\n",
    "    print(len(tokens1 + tokens2 + tokens3))\n",
    "    print(len(tokens1 + tokens2 + tokens3 + tokens4 + tokens5 + tokens6))\n",
    "    print(len(set(tokens1 + tokens2 + tokens3 + tokens4 + tokens5 + tokens6)))\n",
    "    \n",
    "    return tokens1\n",
    "# token을 어떤걸 써야하나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구두점 및 불용어 등등 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filecontent(file):\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def makePattern():\n",
    "    pattern = dict()\n",
    "\n",
    "    # 구두점\n",
    "    pattern1 = re.compile(r'[{0}]'.format(re.escape(punctuation)))\n",
    "    pattern['punc'] = pattern1\n",
    "    # corpus = pattern1.sub(' ',corpus)\n",
    "\n",
    "    # 불용어\n",
    "    pattern2 = re.compile(r'[A-Za-z0-9]{7,}')\n",
    "    pattern['stop'] = pattern2\n",
    "    # corpus = pattern2.sub(' ',corpus)\n",
    "\n",
    "    # 이메일\n",
    "    # pattern3 = re.compile(r'\\w{2,}@\\w{3,}(.\\w{2,})+')\n",
    "    pattern3 = re.compile(r'\\w{2,}@(.?\\w{2,})+')\n",
    "    pattern['email'] = pattern3\n",
    "    # corpus = pattern3.sub(' ',corpus)\n",
    "\n",
    "    # 도메인\n",
    "    pattern4 = re.compile(r'(.?\\w{2,}){2,}')\n",
    "    pattern['url'] = pattern4\n",
    "    # corpus = pattern4.sub(' ',corpus)\n",
    "\n",
    "    # 한글 이외\n",
    "    pattern5 = re.compile(r'[^가-힣0-9]+')\n",
    "    pattern['nonkorean'] = pattern5\n",
    "    # corpus = pattern5.sub(' ',corpus)\n",
    "\n",
    "    # WhiteSpace\n",
    "    pattern6 = re.compile(r\"\\s{2,}\")\n",
    "    pattern['whitespace'] = pattern5\n",
    "    # corpus = pattern6.sub(' ',corpus)\n",
    "    \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# content = filecontent(fileids('./news_crawl_project/')[-2])\n",
    "\n",
    "pattern = makePattern()\n",
    "\n",
    "def punc_stop(file):\n",
    "    for _ in ['email', 'punc', 'stop','whitespace']:\n",
    "        file = pattern[_].sub(' ',file)\n",
    "    return file\n",
    "\n",
    "def indexing(file):\n",
    "    indexTerm1 = defaultdict(int)\n",
    "    indexTerm2 = defaultdict(int)\n",
    "    indexTerm3 = defaultdict(int)\n",
    "    indexTerm4 = defaultdict(int)\n",
    "    indexTerm5 = defaultdict(int)\n",
    "    indexTerm6 = defaultdict(int)\n",
    "\n",
    "    for term in word_tokenize(file):\n",
    "        indexTerm1[term] += 1 # 원시어절\n",
    "    \n",
    "    for _ in indexTerm1:\n",
    "        for t in ma.pos(_):\n",
    "            indexTerm2[t] += 1 # 원시형태소+품사\n",
    "            if len(t[0]) > 1: # 음절 길이로 정규화\n",
    "                indexTerm3[t[0]] += 1 # 원시형태소\n",
    "            if t[1].startswith('N'):\n",
    "                indexTerm4[t[0]] += 1 # 명사\n",
    "            for n in ngram(t[0]): # 바이그램\n",
    "                indexTerm5[n] += 1\n",
    "                indexTerm6[n] += 1\n",
    "    \n",
    "    return indexTerm4 #일단은 명사만 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DocTermMatrix -> TermDocMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 진행중\n",
      "200 진행중\n",
      "300 진행중\n",
      "400 진행중\n",
      "500 진행중\n",
      "600 진행중\n",
      "700 진행중\n",
      "800 진행중\n",
      "900 진행중\n",
      "1000 진행중\n",
      "1100 진행중\n",
      "1200 진행중\n"
     ]
    }
   ],
   "source": [
    "def DTM_conversion():\n",
    "    documentList = defaultdict(lambda: defaultdict(int))\n",
    "    idx = 0\n",
    "    for file in fileids('./news_crawl_project/'):\n",
    "        documentList[idx] = indexing(punc_stop(filecontent(file)))\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx, '진행중')\n",
    "    return documentList\n",
    "DTM = DTM_conversion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDM_conversion(DTM):\n",
    "    TDM = defaultdict(lambda:defaultdict(int))\n",
    "    for idx, termList in DTM.items():\n",
    "        for term, freq in termList.items():\n",
    "            TDM[term][idx] = freq\n",
    "    return TDM\n",
    "TDM = TDM_conversion(DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TermDocMatrix -> TermWeightMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log2\n",
    "\n",
    "N = len(DTM)\n",
    "\n",
    "TWM = defaultdict(lambda:defaultdict(float))\n",
    "DVL = defaultdict(float)\n",
    "\n",
    "for idx, termList in DTM.items():\n",
    "    if len(termList) > 0:\n",
    "        maxTF = max(termList.values()) \n",
    "        \n",
    "        for term, freq in termList.items():\n",
    "            TF = freq/maxTF\n",
    "            IDF = log2(N/len(TDM[term]))\n",
    "            TWM[term][idx] = TF*IDF\n",
    "            DVL[idx] += TWM[term][idx]**2\n",
    "        \n",
    "for idx, length in DVL.items():\n",
    "    DVL[idx] = sqrt(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query에서 색인어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def TQM_conversion(query):\n",
    "    \n",
    "    indexTerm1 = defaultdict(int)\n",
    "    indexTerm2 = defaultdict(int)\n",
    "    indexTerm3 = defaultdict(int)\n",
    "    indexTerm4 = defaultdict(int)\n",
    "    indexTerm5 = defaultdict(int)\n",
    "    indexTerm6 = defaultdict(int)\n",
    "\n",
    "    for _ in word_tokenize(query):\n",
    "        for t in ma.pos(_):\n",
    "            indexTerm2[t] += 1 # 원시형태소+품사\n",
    "            if len(t[0]) > 1: # 음절 길이로 정규화\n",
    "                indexTerm3[t[0]] += 1 # 원시형태소\n",
    "            if t[1].startswith('N'):\n",
    "                indexTerm4[t[0]] += 1 # 명사\n",
    "            for n in ngram(t[0]): # 바이그램\n",
    "                indexTerm5[n] += 1\n",
    "                indexTerm6[n] += 1\n",
    "    return indexTerm4\n",
    "\n",
    "query = '서울시에 거래되는 아파트의 전세값은?'\n",
    "TQM = TQM_conversion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'서울시': 1, '거래': 1, '아파트': 1, '전세': 1, '값': 1})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TQM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쿼리 가중치 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWM = defaultdict(float)\n",
    "        \n",
    "alpha = 0.5\n",
    "maxTF = max(TQM.values())\n",
    "for term, ferq in TQM.items():\n",
    "    TF = alpha + (1-alpha)*(freq/maxTF)\n",
    "    DF = len(TWM[term]) if len(TWM[term]) > 0 else 1\n",
    "    IDF = log2(N/DF)\n",
    "    QWM[term] = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'서울시': 4.646162657157894,\n",
       "             '거래': 4.042091333489033,\n",
       "             '아파트': 4.080565481303669,\n",
       "             '전세': 5.897701424153858,\n",
       "             '값': 5.335822536545743})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QWM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도 계산 (Euclidean vs. Cosine) -> 검색결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서이름:270 / 유사도:1.7061\n",
      " 기존 매물도 많은데 얼마 깎아야 팔리나 걱정 촛불집회까지 강남은 신도시와 무관 무덤덤 호가 올라 추격 매수는 주춤 서울 연합뉴스 서미숙 홍국기 기자 3기 신도시 발표후 집을 사겠다는 사람은 종적을 감추고 매물을 얼마나 더 싸게 내놔야 팔리겠냐는 집주인들 문의만 옵니다 수천만원 정도는 우습게 빠질 것 같네요 지난 11일 일산서구 후곡마을에 위치한 한 중개업소 대표 말이다 그는 이 곳은 지난해 9 13대책 이후 거래가 끊겨서 나온지 몇 달 된 물건들도 수두룩한데 3기 신도시 소식을 듣고 누가 집을 사겠느냐 며 지난해 5억원 하던 전용 84 아파트값이 최근 4억2천만 4억3천만원으로 내려왔지만 실거래가 되려면 이보다 더 낮춰야 할 것 이라고 말했다 지난 7일 3기 신도시 발표 이후 일산 파주 인천 서구 등 신도시의 직접적인 영향권에 있는 지역은 마치 찬물을 끼얹은 듯 시장 분위기가 냉랭했다 당장 급매물이 추가로 쏟아지거나 가격이 급락하진 않았지만 매수세가 끊기면서 집값 하락을 걱정하는 목소리가 많다 반면 강남을 비롯한 서울은 평온한 분위기 속에 최근 급매물 소진 이후 이어졌던 추격 매수세는 다소 주춤해진 모습이다 연합뉴스 자료사진 가뜩이나 안좋은데 찬물 끼얹은 수도권 외곽 이번 고양 창릉 등 3기 신도시 건설 계획에 가장 크게 반대하고 있는 일산서구 아파트 시장은 아예 매수세가 실종됐다 일산서구 주엽동의 한 중개업소 대표는 고양 원흥 삼송지구 등 인근 새 아파트 입주로 이 일대가 대규모 베드타운이 됐는데 또다시 일산신도시 절반 수준의 신도시가 들어선다고 하니 누가 집을 사겠느냐 며 신도시 발표 후 매수 문의는 한 통도 없고 기존에 매물을 내놨던 집주인들한테 얼마를 더 낮춰야 집이 팔리겠냐고 걱정하는 전화만 온다 고 분위기를 전했다 인터넷 커뮤니티 등에는 일부 사정이 다급한 매도자들이 1천만 2천만원 이상 가격을 추가로 낮춰 내놨다는 글이 올라오기도 했다 일산서구 일산동의 한 중개업소 사장은 이 곳이 2017년 8 2대책에서 청약조정지역으로 지정되고 지난해 9 13대책의 유탄까지 맞으면서 집값이 역주행하고 있는데 서울과 더 가까운 곳에 신도시를 짓는다고 하니 망연자실한 분위기 라며 가뜩이나 거래도 안되고 가격도 약세였는데 상황이 더 나빠질까 우려된다 고 말했다 실제 국토교통부와 한국감정원에 따르면 일산서구의 주택 거래량은 2017년 7천127건에서 지난해 4천900건으로 31 2 급감했다 같은 기간 정부 규제가 집중된 서울의 주택 거래량이 18만7천797건에서 17만1천50건으로 8 91 줄어든 것에 비해 감소폭이 더 컸다 일산서구는 올해 1 3월 누적 거래량도 721건에 그쳐 작년 1년치 거래량의 14 7 에 불과했다 거래 부진은 집값 하락에 있다 한국감정원 조사 기준 일산서구 아파트값은 2017년 5월 현 정부 출범 이후 0 82 떨어진 것으로 나타났다 같은 기간 1기 신도시인 분당이 16 73 오르고 평촌신도시가 있는 안양 동안구가 7 05 중동신도시가 있는 부천이 5 67 각각 상승한 것과 대조적이다 일산서구 일산동의 한 중개업소 대표는 일산신도시는 늙어가는데 일산보다 교통이 양호한 서울 인근에 새 아파트가 계속해서 들어서니 버텨낼 방법이 없다 며 후곡마을 16단지는 학군 학원시설이 좋아서 한 때 일산의 대치동으로 불리던 인기 아파트였는데 다 옛말이 됐다 고 전했다 일산 지역 주민들은 집값이 역주행중인데 청약조정지역도 해제해달라 고 요구하고 있다 파주 운정신도시도 매수문의가 사라진 채 적막감이 돌았다 운정지구의 한 중개업소 사장은 당장 급매물이 쏟아지는 분위기는 아니지만 신도시 발표후 실수요자들도 일단 관망하는 모습 이라며 간혹 외부에 거주하고 있는 투자수요자들의 걱정스러운 문의전화만 걸려오고 있다 고 말했다 일산신도시 연합회와 파주 운정신도시연합회 주민들은 3기 신도시 건설에 반발해 12일 오후 파주시 운정행복센터 사거리에서 신도시 철회를 요구하는 요구하는 촛불집회를 연다 인천 검단신도시도 지난해 말 인천 계양테크노밸리에 이어 이번에 부천 장대 등 추가 3기 신도시 건설 계획까지 전해지며 더 냉랭한 분위기다 인천도시공사 제공 인천 서구 당하동 힐스테이트 전용면적 85 는 작년 9 13대책 전 3억7천만 3억8천만원이던 매매가격이 최근 3억4천만 3억5천만원으로 떨어진 가운데 실거래가 이뤄지려면 이보다 2천만원 이상 더 낮춰야 할 것으로 현지 중개업소는 보고 있다 검단동의 한 중개업소 사장은 이달부터 새 아파트 분양이 쏟아지면서 미분양 무덤 이 될 것이라는 우려가 많은데 누가 집을 사겠느냐 고 반문하며 집을 팔겠다는 사람은 많은데 살 사람은 실종됐다 고 말했다 인천 서구 금곡동의 한 중개업소 대표는 하나 동남아파트 전용 85 는 지난해 12월 2억2천만원까지 거래됐던 것인데 현재 1억8천만 2억1천만원으로 낮춰도 살 사람이 없다 며 3기 신도시 우려까지 더해져서 추가로 가격 조정을 하지 않으면 거래가 안될 것 이라고 내다봤다 강남은 무덤덤 강북 일부 신도시 효과 지켜보자 관망 서울지역은 3기 신도시 발표에 아직 무덤덤하다 서울 은평구 불광동의 한 중개업소 대표는 대부분 그린벨트인 고양 창릉 등 3기 신도시의 분양가가 주변 시세보다 싸게 나온다면 서울 서북부 지역의 주택 수요가 분산되는 효과는 있을 것 이라며 그러나 당장 이 일대 아파트값에 영향을 주진 않고 있다 고 전했다 이 중개업소 관계자는 오히려 서울보다는 일산신도시나 파주지역 주민들이 창릉쪽으로 내려올 수도 있어서 서울의 영향은 좀 더 지켜봐야 한다 고 진단했다 노원구 상계동의 한 중개업소 사장도 최근 시세보다 싼 급매물이 일부 팔려나가고 급매를 찾는 사람들도 있었는데 신도시 발표후 일단 매수문의가 줄어들긴 했다 며 신도시의 직접적인 영향이라기보다는 일단 공급계획이 발표됐으니 파장을 좀더 지켜보겠다는 기류 라고 말했다 연합뉴스 자료사진 집값 급등으로 3기 신도시 건설 계획의 도화선이 됐던 서울 강남권은 정작 신도시 발표에 무관심한 반응이다 다만 추격 매수세는 다소 주춤해진 분위기다 서초구 반포동의 한 중개업소 대표는 이번 3기 신도시 위치가 강남 대체지역도 아니고 매도 매수 예정자들 모두 별 관심없어 한다 며 다만 대책의 파장을 살피려는 것인지 최근 호가가 올라서인지 매수 대기자들이 잠시 관망하는 분위기 라고 했다 서초구 반포동의 대표 재건축 단지인 반포주공 1 2 4주구나 이주중인 신반포3차 경남아파트 등은 급매가 소진되며 호가가 강세다 서울 강남구 압구정동 현대 대치동 은마 송파구 잠실 주공5단지 등지도 최근 가격이 오르면서 이달 초에 비해 매수 문의가 다소 줄었다 이들 아파트는 지난 3 4월 급매물이 대거 팔려나가면서 가격이 직전 최저가 대비 1억5천만 2억원 이상 회복했다 송파구 잠실 주공5단지 인근의 중개업소 대표는 호가가 너무 올라서인지 금주들어 분위기가 조용하다 며 신도시 영향이라기보다는 가격이 다시 오른 데 대한 부담감으로 보인다 고 말했다 끝 이 시각 많이 본 기사 남은 음식물 절대 돼지에게 주면 안되는 이유 손혜원 나경원 문빠 달창 모르고 쓴게 더 한심 아파트서만 운전 거짓말 배우 음주 들통 벌금형 난 미대 나온 소방관 불 끄던 손으로 벽화 척척 손흥민 토트넘 팬클럽 선정 최고의 선수 골 흽쓸어 하늘도 무심하지 산불에 돌반지 기부한 의인의 비극 방송인 장성규 씨 투어 대회 도중 기권은 왜 유시민 노 대통령 사저 아방궁 이라 공격 용서안돼 코리안 특급 박찬호 요즘은 커쇼가 류현진 보고 배울걸요 스승의날 우린 감사하지 않아요 편지 쓴 학생들 연합뉴스 앱 지금 바로 다운받기 1 8 1 8 \n",
      "문서이름:198 / 유사도:1.4956\n",
      " 창릉 신도시 발표에 일산 파주 주민들 분노의 소리 커져온라인 카페 김현미 장관 공식 블로그에도 성토 잇따라 고양 뉴시스 고범준 기자 정부가 고양시 창릉동과 부천시 대장동을 3기 신도시로 추가 선정했다 국토교통부는 수도권 주택 30만호 공급안 제3차 신규택지 추진 계획 을 발표 3기 신도시는 고양시 창릉동 813만 3만8000가구 부천시 대장동 343만 2만가구 으로 결정됐다 사진은 7일 오후 경기도 고양시 창릉동 일대 모습 2019 05 07 고양 뉴시스 이경환 기자 서울 집 값 잡으려고 일산은 폭락시킵니까 일산 주민들은 국민도 아니랍니까 정부가 경기 고양시 창릉지구를 3기 신도시로 추가 지정하면서 인접한 파주 운정 지역과 기존 1 2기 신도시 주민들의 불만 목소리가 날로 높아지고 있다 파주 운정3지구 등 2기 신도시 분양도 아직 남아 있는 상태여서 공급과잉에 대한 우려 목소리도 커지는데다 30년 가까이 노후화 돼 재건축을 기대했던 일산신도시 주민들은 집 값 회복도 힘들 것으로 보고 있다 특히 일산신도시 등에 비해 서울 접경과 1 이내인 고양 창릉에 새 아파트라는 장점을 갖춘 3만8000만여 가구에 달하는 물량이 쏟아지면 이 일대로 주택수요가 집중 돼 주변 지역은 직격탄을 맞을 것으로 예상된다 이를 대변하듯 각종 지역 온라인 카페에서는 성토의 목소리와 항의집회 등 집단행동도 불사하겠다는 글이 잇따르고 있다 가입자 1152명의 일산넷에는 각 동 대표 통장이 나서고 전 아파트는 현수막을 걸어 사회적 이슈로 국민들이 관심을 갖게 해야 한다 며 오는 12일 오후 6시 30분 운정행복센터 사거리 앞 운정연과 연합하여 집회합니다 등의 글이 게재됐다 580여명이 가입한 후곡방에서도 일산이 슬럼화되지 않도록 일어나야 한다 고 독려하며 행동하지 않으니 이런 사태가 벌어진 것 이라고 지적하는 글도 올라왔다 서울 뉴시스 3기 신도시 주택공급계획 인포그라픽 2019 05 07 제공 국토교통부 또 이 카페에서는 11일 오후 7시 일산호수공원 분수대에서 집회하자는 제안도 했다 김현미 장관 공식블로그에는 자기 지역구 등에다 을 꽃네요 속에서 천불이 나니 그 철판으로 다시는 나올 생각하지 마세요 일산을 슬럼화 만들려고 작정하신 듯 본인 지역구를 제물로 삼았군요 라는 불만의 글이 잇따라 게재되고 있다 한 네티즌은 3억원도 안되는 집이 1년에 3000만원씩 내려 이제 내놔도 거래 근처도 못가고 오늘도 2시간 걸려 출근한다 며 당신을 뽑아 준 내 손가락이 원망스럽다 는 성토의 글도 눈에 띄었다 오전 11시 현재 청와대 국민청원 홈페이지에 3기 신도시 고양 지정 일산신도시에 사망선고 대책을 요구합니다 라는 제목의 청원글에는 모두 9060명이 참여했다 한편 한국감정원에 따르면 지난해 9 13대책 후 지난달까지 서울의 아파트값이 0 04 하락하는 동안 2기 운정 신도시가 자리잡은 파주의 아파트값은 1 25 나 떨어졌다 같은 기간 1기 일산 신도시 권역인 일산동구는 0 54 일산서구는 0 71 각각 하락했다 뉴시스 빅데이터 주가시세표 바로가기 뉴시스 페이스북 트위터 한지성 인천고속도로 사고 전 차량 뒤에서 허리 숙여 남편은 음주상태 부적 주겠다 점 봤던 여성 성폭행한 무속인 징역 6년 성매수남이 화대 훔쳐가자 강간당했다 허위신고 20대 집유 성적 호기심에 출근시간대 특정 신체 노출 공무원 입건 환자 성폭행 혐의 정신과 의사 수사 그루밍 성폭력 주장 1 8 1 8 \n",
      "문서이름:685 / 유사도:1.4626\n",
      " 서울 연합뉴스 박의래 기자 국가인권위원회는 후견 판결을 받은 정신장애인이 금융기관을 이용할 때 후견인 동행을 요구하는 것과 인터넷뱅킹 등 비대면 거래를 허용하지 않는 것은 장애인 권리를 지나치게 제한하는 것이라고 판단했다 13일 인권위에 따르면 씨는 지난 2월 한정후견 판결을 받았다 이후 은행은 씨의 금융서비스 이용 시 100만원 미만 거래만 창구를 통해 혼자 거래할 수 있고 그 이상 금액은 후견인 동행을 요구했다 또 인터넷뱅킹이나 스마트뱅킹 현금자동입출금기 등 비대면 거래도 제한했다 은행은 한정 후견인의 동의 의사를 명확히 확인하기 위해 동행을 요구했고 비대면 거래를 제한한 것은 금융사고 발생 위험 때문이라고 설명했다 그러나 인권위는 이와 관련된 판례를 근거로 30일 이내 100만원 이상 거래 시 은행은 후견인의 동의 를 받도록 했지 동행 을 요구하도록 하는 것은 아니라고 설명했다 그러면서 한정 후견인의 동행 을 요구하고 100만원 미만 거래 시에도 은행에 직접 와서 대면 거래를 하라고 하는 것은 장애인의 금융 활동을 과도하게 제한하는 것 이라고 판단했다 또 은행은 금융사고 발생 위험을 방지하거나 최소화하는 기술적 시스템 장치를 마련해 휴일 등 대면 거래가 불가능한 상황에서 장애인이 금융거래를 할 수 있는 방안을 모색할 책임이 있다고 봤다 다만 인권위는 이 사건이 법원에서 소송 진행 중이어서 국가인권위원회법에 따라 각하했다 대신 금융감독원장에게 개선책 마련이 필요하다는 의견을 표명했다 \n",
      "문서이름:1127 / 유사도:1.3618\n",
      " 서울 연합뉴스 김지헌 기자 전국적인 시내버스 파업이 가시화하는 가운데 서울시는 파업에 이르지 않도록 노력하겠다고 밝혔다 서울시 김의승 대변인은 13일 정례 브리핑에서 시민의 발인 서울 시내버스가 멈추는 일은 없도록 끝까지 최선을 다하겠다 고 말했다 김 대변인은 시는 지난해부터 운전인력 300명 추가 채용과 운행 횟수 감소 등 탄력근로 방식으로 52시간제 도입을 준비해 현재 주당 평균 근로시간이 47 5시간 이라며 현재 파업 의제인 준공영제나 52시간제에 시는 선도적으로 대비해왔고 근로조건이나 처우도 전국 최고 수준 이라고 말했다 이어 근로조건 향상과 시민부담 최소화 원칙에 따라 14일 있을 지방노동위원회의 2차 조정을 통해 원만하게 노사 간 합의가 이뤄지도록 할 것 이라며 이런 노력에도 혹시라도 있을 파업에 대비해 지하철 증편과 운행시간 연장 택시 부제 해제 등 비상수송대책도 준비하는 중 이라고 덧붙였다 버스요금 인상 가능성에는 선을 그었다 서울시버스노동조합은 5 9 임금 인상 정년 연장 학자금 등 복지기금 연장 등 비용 상승 요소를 제기한 상태다 여기에 서울 인천시와 공동으로 수도권통합환승할인제를 시행하는 경기도가 지속해서 서울시에 요금 동반 인상을 요구해오고 있다 김 대변인은 협상 과정에서 시가 가진 안을 구체적으로 말할 수는 없다 면서도 경기도만 요금을 올리는 방안도 가능하다 서울시에 인상할 요인이 있어야 올리는 것 이라고 말했다 시의 한 관계자는 경기도가 환승할인제로 묶여 있어서 서울이 함께 요금 인상을 해야 한다고 하는데 경기도의 인상분은 사후정산으로 얼마든지 돌려줄 수 있어 이유가 될 수 없다 고 강조했다 아울러 경기도 입장만 고려해 인상 요인이 없는 서울시도 함께 올리자고 하는 것은 시민들에게 명분이 없을 뿐만 아니라 결국 스스로 해결해야 할 문제를 다른 지역에 전가하는 것으로밖에 볼 수 없지 않으냐 고 잘라 말했다 서울버스노조의 지난 9일 조합원 파업 찬반 투표 결과 재적 조합원 대비 찬성률 89 3 가 나와 파업이 가결됐다 이에 따라 서울버스노조는 지노위 조정이 최종 불발되면 한국노동조합총연맹 산하 전국자동차노동조합총연맹이 예고한 대로 15일부터 전국 버스노조와 함께 파업에 들어간다 3월 말 기준 서울 시내 전체 버스회사 마을버스 제외 는 총 65개 노선 수는 354개 차량 대수는 7천405대다 \n",
      "문서이름:677 / 유사도:1.3303\n",
      " 서울 뉴스1 이헌일 기자 수도권이 함께 버스 요금을 올려야 한다는 경기도와 정부 더불어민주당 등 요구에 서울시는 요금인상 요인이 없다는 입장을 다시 한번 강조했다 김의승 서울특별시 대변인은 13일 오전 서울시청 정례브리핑 뒤 기자들과 만나 서울시는 버스 요금 인상요인이 없는데 경기도에서 필요하다고 해서 시민의 부담을 늘릴 순 없다 며 명분도 없이 어떻게 요금을 올릴 수 있겠나 라고 말했다 서울과 경기도 부산 등 전국 11개 시도 버스노조는 15일 총 파업에 돌입한다고 예고한 상황이다 이들은 주 52시간 도입에 따른 임금 보전 준공영제 시행 등을 요구하고 있다 다만 서울시는 이미 2004년 준공영제를 도입 버스 업체의 적자를 보전해주고 있다 또 지난해부터 기사 약 300명을 추가로 고용하고 운행횟수를 줄이는 등 주 52시간 도입을 준비해 다른 시도와 상황이 다르다는 설명이다 현재 서울 버스는 주당 평균 근로시간 47 5시간 평균 월 임금 422만원으로 전국 최고 수준이다 이에 대비가 부족했던 경기도 때문에 서울시민이 피해를 입을 수는 없지 않느냐는 것이 서울시의 판단이다 김 대변인은 서울시는 주 52시간 도입을 착실히 준비했고 이미 업계 최고 수준 처우를 갖췄다 며 다른 시도 버스 노조에서는 서울시처럼 해달라고 요구하고 있는 상황 이라고 말했다 경기도는 수도권이 통합환승제로 묶여 있기 때문에 경기도만 단독으로 요금을 올릴 수는 없다는 입장이다 그러나 서울시는 이미 환승에 따른 사후정산 시스템이 갖춰져 있기 때문에 경기도에서만 요금을 올려도 사후정산을 통해 경기도의 몫을 계산할 수 있다고 판단한다 시는 쟁의조정기간인 14일까지 노조와 협의를 지속해 합의를 이끌어낼 계획이다 노조의 요구 가운데 합리적으로 수용가능한 부분을 검토해 시민의 발 인 버스가 멈추는 일이 없도록 최대한 노력하겠다는 방침이다 서울 버스노조는 임금 5 98 인상 주5일 근무 확립 정년 연장 61 63세 학자금복지기금 지급기간 연장 등을 요구하고 있다 앞서 12일 김현미 국토교통부 장관과 이재갑 고용노동부 장관은 정부서울청사에서 버스노조 노동쟁의 신청에 따른 합동 연석회의 를 열고 현실적으로 시내버스 요금 인상이 필요하다 며 각 지방자치단체가 요금인상을 포함한 재원마련 방안을 내놓는 등 버스업계와 지자체 중앙정부 모두 고통을 분담하는 자세 를 가져야 한다는 결론을 내놓았다 여당도 서울시에 버스 요금 인상을 설득하는 것으로 알려졌다 \n"
     ]
    }
   ],
   "source": [
    "candidateList = defaultdict(float)\n",
    "for term, weight1 in QWM.items():\n",
    "    for doc, weight2 in TWM[term].items():\n",
    "        innerProduct = weight1 * weight2\n",
    "        candidateList[doc] += innerProduct\n",
    "        \n",
    "for doc, sim in candidateList.items():\n",
    "    candidateList[doc] = sim/DVL[doc]\n",
    "    \n",
    "K = 5\n",
    "\n",
    "for doc, sim in sorted(candidateList.items(), key=lambda x:x[1], reverse=True)[:K]:\n",
    "    print('문서이름:{0} / 유사도:{1:.4f}'.format(doc, sim))\n",
    "    print(punc_stop(filecontent(fileids('./news_crawl_project/')[doc])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서이름:115 / 거리:4.0010\n",
      " 국내에서 유튜브를 가장 많이 보는 연령대는 50대 이상 인 것으로 나타났다 50대 이상 유튜브의 사용시간은 1년 사이 두 배로 늘었다 앱 분석업체 와이즈앱이 지난 4월 전국 안드로이드 스마트폰 사용자 3만3000명을 조사한 결과 이 같이 나타났다고 14일 밝혔다 50대 이상의 유튜브 사용시간은 101억분으로 가장 많았다 이는 지난해 4월 51억분에서 두 배가량 늘어난 수치다 다음으로 10대 89억분 20대 81억분 30대 61억분 40대 57억분 등 순이었다 1인 평균 시청 시간은 10대가 평균 1895분 월 31시간 35분 으로 가장 길었다 20대 1625분 가 그 뒤를 이었고 50대 이상은 145분으로 30대 988분 와 40대 781분 보다 높았다 유튜브 앱 사용시간은 총 388억분으로 지난해 4월 258억분보다 50 늘었다 카카오톡 사용시간은 225억분 네이버 153억분 페이스북 42억분 등 순이었다 이들 앱의 사용시간은 지난해보다 각각 19 21 5 성장했지만 유튜브와의 격차는 더욱 벌어졌다 곽희양 기자 오늘의 인기뉴스 김민아 칼럼 황교안적 인 너무도 황교안적 인이낙연 총선 등판 기정사실화하나박해미 음주 사고 황민과 결국 이혼 25년 결혼생활 종지부미중 관세확전에 세계증시 패닉 공포지수 30 급등 단독 경찰의 청와대 보고용 2016년 총선 보고서 본 유명 선거 컨설턴트 우린 억만금 줘도 이런 건 못 만든다 최신 뉴스 두고 두고 읽는 뉴스 인기 무료만화 경향신문 무단전재 및 재배포 금지 1 8 1 8 \n",
      "\n",
      "문서이름:113 / 거리:4.1545\n",
      " 총 이용시간 258억 분에서 388억 분 50 키뉴스 이길주 기자 앱 리테일 분석서비스 와이즈앱이 지난 4월 한국 안드로이드 스마트폰 사용자의 세대별 사용 현황을 발표했다 전 세대를 합쳐 한국인이 가장 오래 사용한 앱은 유튜브로 4월 한달 총 사용시간 388억 분을 이용했다 그 뒤를 카카오톡 225억 분 네이버 153억 분 페이스북 42억 분의 순이었다 한국인이 오래 사용하는 4대 앱의 사용시간은 모두 증가했다 유튜브는 작년 4월 총 이용시간 258억분에서 올 4월에는 388억 분으로 50 늘었다 카카오톡은 작년 4월 총 이용시간 189억분에서 올 4월 225억 분으로 19 성장했다 네이버는 작년 4월 총 이용시간 126억분에서 올 4월 153억 분으로 21 증가했다 페이스북은 작년 4월 총 이용시간 40억분에서 올 4월 42억 분으로 5 늘었다 한국인 오래 사용하는 앱 유튜브 카카오톡 네이버 페이스북 순으로 나타났다 표 와이즈앱 유튜브의 경우 1인당 평균 사용시간도 작년 4월 882분에서 올 4월에는 1188분으로 35 늘어 주요 앱의 1인당 평균 사용시간 증가율 중 가장 높은 증가율이었다 월 사용자 도 작년 4월 2924만 명에서 올 4월에는 3 271만명으로 12 늘어 3580만명의 카카오톡에 이어 한국에서 두 번째로 사용자 수가 많았다 유튜브는 10대 20대 30대 40대 50대 이상의 모든 세대에서 가장 오래 사용한 앱이었다 10대는 1인당 평균 1 895분을 이용하여 월 31시간 35분 1인당 평균 사용시간이 가장 긴 세대였다 20대는 1인당 평균 1652분을 이용하여 월 27시간 32분 1인당 평균 사용시간이 두 번째로 길었다 50대 이상의 세대는 30대 988분 와 40대 781분 보다 1인당 평균사용시간이 긴 1045분 월 17시간 25분 으로 나타났다 50대 이상의 1인당 평균 사용시간은 10대와 20대 보다는 작지만 이용자가 크게 늘면서 50대 이상의 총 사용시간은 전세대에서 1위인 101억 분을 기록했다 1 8 1 8 \n",
      "\n",
      "문서이름:1110 / 거리:5.2548\n",
      " 패스트트랙 신속처리 대상 안건 에 오른 검경 수사권 조정 관련 법안에 문제를 제기하고 있는 검찰이 수사지휘권 확보를 위해 특별수사 단계적 축소안 을 제안하고 있는 것으로 확인됐다 국회 사법개혁특별위원회 위원 가운데에도 검찰의 직접 수사를 최대한 줄여야 한다고 보는 이가 많아 입법 과정에서 논의가 어떻게 전개될지 관심이 쏠린다 특별수사 아웃소싱 정부 관계자는 최근 한겨레 와 한 통화에서 5대 고검과 인천지검 정도에만 특수부를 남기고 나머지는 모두 없애는 방향으로 갈 것 이라며 특별수사를 아웃소싱한다면 제일 먼저 가능한 분야가 마약 이라고 말했다 또 다른 정부 관계자는 마약 조세 금융 등 전문수사영역을 차례로 독립시키고 장기적으로는 검찰이 아예 직접수사를 하지 않는 방향으로 가려고 한다 고 했다 특별수사 단계적 축소안은 문무일 검찰총장 취임 뒤 검찰이 자체 개혁안의 하나로 여러차례 밝힌 바 있다 검찰이 수사권 논의 국면에서 기존안을 적극 활용해 수사지휘권을 지켜내겠다는 전략으로 풀이된다 문 총장은 지난 7일 패스트트랙에 오른 검경 수사권 조정 관련 법안에 문제를 제기하며 수사에 대한 사법적 통제와 더불어 수사의 개시 종결이 구분돼야 국민의 기본권이 온전히 보호될 수 있다고 생각한다 고 밝혔다 이상민 국회 사법개혁특별위원장도 지난 9일 한겨레 와 만나 비슷한 입장을 밝혔다 이 위원장은 현재 발의된 법안을 보면 검찰의 직접 수사를 완전히 없앤 것도 아니고 수사지휘권도 애매하게 돼 있다 며 검찰이 직접 수사하지 않되 경찰조직에 전체적으로 수사를 다 맡기지 말고 범죄수사청 특별범죄수사처 등 범죄 유형별로 수사기관을 전문화할 필요가 있다 고 말했다 다만 정부와 여권에서는 이런 움직임을 부정적으로 보는 시각도 있다 정부 핵심 관계자는 경찰이 갖게 된 1차 수사종결권 을 일부 보완하자는 논의는 할 수 있지만 마약 금융 조세 부문 수사를 검찰에서 떼어내는 대가로 기존 수사지휘권을 부활시키자는 건 애초 합의를 다 깨트리는 것이라 불가능하다 고 말했다 경찰의 수사종결권에 대한 견제장치를 마련하는 것과 검찰의 직접 수사를 줄이는 것은 개혁 차원에서 별도로 추진돼야 할 사안이지 맞바꿀 사안이 아니라는 점을 분명히 한 것이다 수사종결권 둘러싼 검경 신경전 검경 수사권 조정과 관련해 지난달 29일 패스트트랙에 오른 형사소송법 개정안과 검찰청법 개정안은 지난해 6월 발표된 정부안의 뼈대인 경찰의 1차적 수사권 및 수사종결권 사건 송치 전 검사의 수사지휘 폐지 등을 담고 있다 이와 관련해 검경이 가장 첨예하게 다투고 있는 부분은 경찰의 수사종결권을 검찰이 견제할 수 있는가 하는 문제다 경찰은 우리가 불기소 결론을 내려도 검찰이 사건기록을 보내달라고 할 수 있고 재수사 요청권도 가진다 며 통제 장치가 있다고 주장한다 반면 검찰은 형소법 개정안에 검찰의 보완수사 요구를 이행하는 전제로 정당한 이유가 없는 한 이라는 문구가 추가된 점 등을 들어 사실상 수사지휘권이 폐지됐다 고 반발하고 있다 요구를 받은 경찰관이 정당한 이유가 있다 며 보완수사를 거부해도 이를 강제할 방법이 없다고 보는 것이다 검찰이 해당 경찰관의 징계나 직무배제를 요구할 수 있지만 이 또한 경찰이 따르지 않으면 그만이라는 게 검찰의 주장이다 검찰 관계자는 현재 안은 경찰의 1차 종결 검찰의 1차 재수사 요구 경찰의 2차 종결 검찰의 2차 재수사 요구 가 무한 반복될 수도 있다 고 말했다 김원철 서영지 기자 \n",
      "\n",
      "문서이름:109 / 거리:5.6049\n",
      " 유튜브가 한국인을 사로잡았다 10대부터 50대 이상까지 모두 가장 오래 이용한 앱으로 유튜브가 꼽혔을 정도다 앱 리테일 분석서비스 와이즈앱이 국내 안드로이드 스마트폰 사용자 3만3000명의 세대별 사용현황을 분석한 결과 전세대 한국인이 가장 오래 사용한 앱은 유튜브로 드러났다 한국인이 가장 오래 사용하는 앱으로 유튜브가 꼽혔다 인터넷 브라우저 앱과 게임앱은 집계에서 제외됐다 한국 안드로이드 스마트폰 사용자는 지난 4월 한달간 총 388억분동안 유튜브를 시청했다 카카오톡은 225억분 사용했으며 네이버는 153억분 페이스북은 42억분이었다 유튜브 이용시간은 전년 4월 258억분 대비 50 성장하며 다른 앱보다 사용시간이 크게 늘었다 카카오톡은 19 네이버는 21 페이스북은 5 증가했다 유튜브 1인당 사용시간도 지난해 4월 882분에서 올해 4월 1188분으로 35 증가했다 유튜브는 10대 20대 30대 40대 50대 이상 등 모든 세대에서 가장 오랜 시간 사용한 앱이었다 지난달 기준 1인당 유튜브 이용시간이 긴 연령대는 10대 1895분 20대 1652분 50대 이상 1045분 30대 988분 40대 781분 순이었다 유튜브 총 사용시간은 50대가 1위로 101억분을 기록했다 조선닷컴 핫 뉴스 3조6000억 투자 신동빈 초청 환영 트윗 올린 트럼프 집단폭행의 결과 중학생 추락사 가해 10대 전원 실형 1년 손놓고 있다가 버스대란 닥치자 결국 세금 카드 상상불가의 능력 체형편견 극복한 류현진에 경의 박해미 황민과 25년간의 결혼 생활 끝 협의 이혼 조선닷컴 바로가기 조선일보 구독신청하기 조선일보 무단 전재 및 재배포 금지 안소영 기자 1 8 1 8 \n",
      "\n",
      "문서이름:934 / 거리:5.6066\n",
      " 머니투데이 베이징 중국 진상현 특파원 600억 달러 제품에 추가관세 5 25 로 인상 2000억 달러 제품에 25 적용하는 에 규모 세율서 큰 격차 부에노스아이레스 신화 뉴시스 1일 현지시간 아르헨티나 부에노스아이레스에서 도널드 트럼프 미국 대통령과 시진핑 중국 국가주석이 만찬 회동에서 악수하고 있다 악화일로를 걷던 미중 무역갈등은 이날 회동을 통해 휴전으로 일단 봉합됐다 2018 12 02 중국 정부가 13일 이하 현지시간 600억 달러 규모의 미국산 수입품에 대한 추가 관세율을 최대 25 로 인상하는 보복 조치를 발표했다 하지만 2000억 달러 중국산 수입품에 대한 추가 관세를 25 로 일괄 인상한 미국에 비해 규모나 인상폭 모두 크게 못미친다 미국에 비해 수입 규모가 자체가 적어 관세 실탄 이 부족한 중국의 고민이 고스란히 드러났다는 평가다 중국 국무원 관세 세칙위원회는 이날 밤 다음달 1일부터 600억달러 상당의 미국 상품 총 5140 품목에 5 25 의 관세를 부과할 것 이라고 밝혔다 앞서 5 10 의 차등 관세가 적용되던 것을 5 25 로 차등 폭을 높인 것이다 보복 관세가 부과되는 품목 가운데 2493개 품목에는 25 1078개 품목은 20 974개 품목은 10 595개 품목은 5 추가 관세가 각각 부과된다 미국은 지난해 7월과 8월 각각 340억달러 160억달러 규모의 중국산 제품에 25 의 추가관세를 부과했다 중국은 같은 시기 동일한 규모와 관세율로 반격했다 이어 미국이 지난해 9월 다시 2000억달러 어치 중국산 수입품에 관세 10 를 추가로 매기자 중국은 600억달러 규모 미국산 제품에 5 10 의 차등 추가 관세로 대응했다 당시 중국은 미국이 2000억 달러 제품에 적용하는 추가 관세를 25 로 인상하면 그에 상응하는 조치를 취하겠다고 밝혔었다 중국의 새로운 보복 조치가 적용되더라도 미국이 총 2500억 달러 296조8750억원 어치 중국산 수입품에 대해 25 의 추가 관세를 적용하는 반면 중극은 미국산 수입품 500억 달러 59조3750억원 에는 25 600억 달러 71조2500억원 에는 5 25 의 추가 관세가 각각 적용하는데 그친다 직접적인 피해는 시간이 갈수록 중국측이 더 커진다는 얘기다 문제는 앞으로다 미국이 남은 약 3000억 달러 356조2500억원 규모의 중국산 수입품에도 25 의 관세를 매기겠다고 밝혔지만 중국은 이에 대한 대응 계획은 아직 밝히지 않고 있다 그도 그럴 것이 지난해 기준으로 중국의 미국산 수입 규모는 1203억 달러 142조8563억원 로 관세 부과를 하지 않은 미국산 수입품이 100억 달러 안팎 밖에 남지 않았다 사실상 실탄이 없다는 얘기다 미국의 지난해 중국산 수입 규모는 총 5395억 달러 640조6563억원 에 달했다 중국이 이번 관세 적용 시점을 다음달 1일 부터로 늦춰 잡은 것도 미국을 최대한 덜 자극하면서 그 사이 협상을 통해 문제를 풀려는 의지가 담겼다는 관측이다 도널드 트럼프 대통령은 13일 오전 자신의 트위터를 통해 중국은 보복해서는 안 된다 더 나빠지기만 할 뿐 이라고 경고했다 미국도 앞서 2000억 달러 25 추가 관세 적용 시점을 5월10일 오전 0시 1분 이후에 미국으로 출발하는 중국 화물로 잡아 인상된 세율로 관세를 실제 징수하기까지 시차를 뒀다 중국의 관세 실탄이 소진되면서 관세 외에 다른 보복 카드를 꺼내들 가능성도 거론된다 트럼프 대통령의 핵심 지지 기반인 팜벨트 농장지대 를 흔들 수 있는 대두 등 농작물에 대한 고관세 및 수입 제한 중국산 중간 제품의 미국 수출 중단 미국 여행 제한 등이 회자된다 하지만 이는 미국에 새로운 공격 빌미를 제공하고 그나마 이어지고 있는 협상의 끈을 완전히 놓칠 수도 있다는 점에서 선택이 쉽지 않을 것이라는 관측이다 관련기사 돈 벌어봤냐 는 황교안 비판에 임종석 재밌는 얘기 해드릴까 세계일주하다 피랍된 40대 여성 미스터리 5가지 정체불명 로즈데이 데이 어디까지 알고 계신가요 리포트 가지 말라는데 왜 가 피랍 구출 딜레마 다림질 바지를 뒤집어야 하는 이유는 청소년성폭행 6년 복역 후 4개월만에 또 칼 들고 특수강간 징역 5년 알짜 매물 아시아나 인수 망설이는 속내는 민아 팀 탈퇴 설현 포함 5인 체제로 팀 유지 전문 속보 3000억달러 관세 목록 공개 경찰차 쿵 시속 130 만취운전 도심 추격전 베이징 중국 진상현 특파원 1 8 1 8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidateList = defaultdict(float)\n",
    "for term, docList in TWM.items():\n",
    "    for doc, weight1 in docList.items():\n",
    "        weight2 = QWM[term]\n",
    "        candidateList[doc] += (weight1 - weight2)**2 \n",
    "        \n",
    "for doc, sim in candidateList.items():\n",
    "    candidateList[doc] = sqrt(sim)\n",
    "\n",
    "# for doc in DTM:\n",
    "#     print(doc, len(punc_stop(filecontent(fileids('./news_crawl_project/')[doc])).split()), len(DTM[doc]), sum(DTM[doc].values()))\n",
    "\n",
    "K = 5\n",
    "for doc, sim in sorted(candidateList.items(), key=lambda x:x[1])[:K]:\n",
    "    print('문서이름:{0} / 거리:{1:.4f}'.format(doc, sim))\n",
    "    print(punc_stop(filecontent(fileids('./news_crawl_project/')[doc])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precesion(정확율) : 검색결과는 최대한 많이,\n",
    "## Recall(재현율) : 유사도는 최대한 높이\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
