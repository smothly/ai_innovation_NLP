{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA - Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "a = 0.1\n",
    "b = 0.1\n",
    "K = 3\n",
    "\n",
    "docTermTopicMat = list()\n",
    "Vocaburary = list()\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "for d in documents:\n",
    "    termTopic = list()\n",
    "    for t in d:\n",
    "        termTopic.append([t.lower(), random.randrange(K)])\n",
    "        Vocaburary.append(t.lower())\n",
    "    docTermTopicMat.append(termTopic)\n",
    "Vocaburary = list(set(Vocaburary))\n",
    "        \n",
    "M = len(docTermTopicMat)\n",
    "N = len(Vocaburary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['libsvm', 0], ['regression', 0], ['support vector machines', 2]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocaburary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicTermMatrix = defaultdict(lambda:defaultdict(int))\n",
    "docTopicMatrix = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "for i, termTopic in enumerate(docTermTopicMat):\n",
    "    for row in termTopic:\n",
    "#         row[0] #단어\n",
    "#         row[1] #토픽\n",
    "        # (1 / 분자)\n",
    "\n",
    "        \n",
    "        # (1) (1 / 분모) k번째 토픽에서, r번째 고유어가 몇 번\n",
    "        topicTermMatrix[row[1]][row[0]] += 1\n",
    "        # docTopicMatrix[m번째문서][k번째토픽] = 몇개의 단어\n",
    "        # (2) i번째 문서에서, k번째 토픽에 몇 개의 단어 \n",
    "        docTopicMatrix[i][row[1]] += 1 \n",
    "    \n",
    "#         theta = 문서에 토픽 분포\n",
    "#         phi = 토픽에 단어 분포\n",
    "        \n",
    "        \n",
    "# 1. 문서에 상관없이 각 단어가 어느 토픽에 몇 번 + B 나왔는지\n",
    "# 2. 문서에 상관없이 특정단어가 k번째 토픽에 몇 번 + B나왔는지\n",
    "# 3. m개의문서에서 k번째 토픽에 몇개의 단어 + a 나왔는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicAssign(m, l):\n",
    "\n",
    "    totalCount = sum([f for k in range(K)\n",
    "     for t,f in topicTermMatrix[k].items()\n",
    "     if t == l])\n",
    "#     print(l, totalCount)\n",
    "    probList = list()\n",
    "    for k in range(K):\n",
    "        probList.append(topicLikelihood(k, l) * docLikelihood(m, k))\n",
    "    _sum = sum(probList) * random.random()\n",
    "    \n",
    "    for i, p in enumerate(probList):\n",
    "        _sum -= p\n",
    "        if _sum <= 0:\n",
    "            k = i\n",
    "            break\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicLikelihood(k, l):\n",
    "    return (topicTermMatrix[k][l] + b) / \\\n",
    "            (sum(topicTermMatrix[k].values()) + (b*N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docLikelihood(m, k):\n",
    "#     print(m, k)\n",
    "    return (docTopicMatrix[m][k] + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = 1000\n",
    "for _ in range(_iter):\n",
    "    for i, termTopic in enumerate(docTermTopicMat):\n",
    "        for row in termTopic:\n",
    "            topicTermMatrix[row[1]][row[0]] -= 1\n",
    "            docTopicMatrix[i][row[1]] -= 1\n",
    "            \n",
    "            k = topicAssign(i, row[0])\n",
    "            \n",
    "            row[1] = k #토픽 새로 assign\n",
    "            topicTermMatrix[row[1]][row[0]] += 1\n",
    "            docTopicMatrix[i][row[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: -1, 0: 7, 2: 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTopicMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {1: defaultdict(int,\n",
       "                         {'hadoop': -1,\n",
       "                          'big data': 0,\n",
       "                          'java': 1,\n",
       "                          'storm': 0,\n",
       "                          'cassandra': 0,\n",
       "                          'nosql': 0,\n",
       "                          'mongodb': 0,\n",
       "                          'scipy': 1,\n",
       "                          'r': 4,\n",
       "                          'machine learning': 0,\n",
       "                          'programming languages': 1,\n",
       "                          'statistics': 3,\n",
       "                          'probability': 3,\n",
       "                          'mahout': 0,\n",
       "                          'neural networks': 0,\n",
       "                          'deep learning': 0,\n",
       "                          'artificial intelligence': 0,\n",
       "                          'python': 3,\n",
       "                          'hbase': 0,\n",
       "                          'spark': 0,\n",
       "                          'postgres': 0,\n",
       "                          'scikit-learn': 1,\n",
       "                          'numpy': 0,\n",
       "                          'statsmodels': 2,\n",
       "                          'pandas': 2,\n",
       "                          'regression': 1,\n",
       "                          'decision trees': 0,\n",
       "                          'libsvm': 0,\n",
       "                          'c++': 2,\n",
       "                          'haskell': 1,\n",
       "                          'mathematics': 1,\n",
       "                          'theory': 1,\n",
       "                          'mapreduce': 0,\n",
       "                          'databases': 0,\n",
       "                          'mysql': 0,\n",
       "                          'support vector machines': 0}),\n",
       "             0: defaultdict(int,\n",
       "                         {'hbase': 3,\n",
       "                          'postgres': 2,\n",
       "                          'scikit-learn': 0,\n",
       "                          'numpy': 1,\n",
       "                          'statsmodels': 0,\n",
       "                          'probability': 0,\n",
       "                          'regression': 0,\n",
       "                          'libsvm': 0,\n",
       "                          'haskell': 0,\n",
       "                          'machine learning': 0,\n",
       "                          'big data': 2,\n",
       "                          'hadoop': 2,\n",
       "                          'java': 2,\n",
       "                          'c++': 0,\n",
       "                          'pandas': 0,\n",
       "                          'mongodb': 2,\n",
       "                          'spark': 1,\n",
       "                          'storm': 1,\n",
       "                          'cassandra': 2,\n",
       "                          'nosql': 1,\n",
       "                          'python': 1,\n",
       "                          'scipy': 0,\n",
       "                          'r': 0,\n",
       "                          'statistics': 0,\n",
       "                          'decision trees': 0,\n",
       "                          'programming languages': 0,\n",
       "                          'mathematics': 0,\n",
       "                          'theory': 0,\n",
       "                          'mahout': 0,\n",
       "                          'neural networks': 0,\n",
       "                          'deep learning': 0,\n",
       "                          'artificial intelligence': 0,\n",
       "                          'mapreduce': 1,\n",
       "                          'databases': 1,\n",
       "                          'mysql': 1,\n",
       "                          'support vector machines': 0}),\n",
       "             2: defaultdict(int,\n",
       "                         {'spark': 0,\n",
       "                          'hbase': 0,\n",
       "                          'python': 0,\n",
       "                          'pandas': 0,\n",
       "                          'statistics': 0,\n",
       "                          'regression': 2,\n",
       "                          'decision trees': 1,\n",
       "                          'c++': 0,\n",
       "                          'mathematics': 0,\n",
       "                          'theory': 0,\n",
       "                          'scikit-learn': 1,\n",
       "                          'neural networks': 2,\n",
       "                          'artificial intelligence': 2,\n",
       "                          'mapreduce': 0,\n",
       "                          'r': 0,\n",
       "                          'statsmodels': 0,\n",
       "                          'deep learning': 2,\n",
       "                          'databases': 0,\n",
       "                          'mysql': 0,\n",
       "                          'support vector machines': 1,\n",
       "                          'hadoop': 0,\n",
       "                          'big data': 1,\n",
       "                          'java': 0,\n",
       "                          'storm': 0,\n",
       "                          'cassandra': 0,\n",
       "                          'nosql': 0,\n",
       "                          'mongodb': 0,\n",
       "                          'postgres': 0,\n",
       "                          'scipy': 0,\n",
       "                          'numpy': 0,\n",
       "                          'probability': 0,\n",
       "                          'machine learning': 2,\n",
       "                          'libsvm': 2,\n",
       "                          'haskell': 0,\n",
       "                          'programming languages': 0,\n",
       "                          'mahout': 1})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicTermMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: defaultdict(int, {1: -1, 0: 7, 2: 0}),\n",
       "             1: defaultdict(int, {1: 0, 2: 0, 0: 5}),\n",
       "             2: defaultdict(int, {2: 0, 0: 2, 1: 4}),\n",
       "             3: defaultdict(int, {1: 5, 2: 0, 0: 0}),\n",
       "             4: defaultdict(int, {1: 0, 0: 0, 2: 4}),\n",
       "             5: defaultdict(int, {2: 0, 1: 6, 0: 0}),\n",
       "             6: defaultdict(int, {1: 4, 2: 0, 0: 0}),\n",
       "             7: defaultdict(int, {0: 0, 2: 4, 1: 0}),\n",
       "             8: defaultdict(int, {2: 4, 1: 0, 0: 0}),\n",
       "             9: defaultdict(int, {0: 4, 2: 0, 1: 0}),\n",
       "             10: defaultdict(int, {2: 0, 0: 0, 1: 3}),\n",
       "             11: defaultdict(int, {0: 0, 2: 2, 1: 2}),\n",
       "             12: defaultdict(int, {0: 0, 2: 0, 1: 3}),\n",
       "             13: defaultdict(int, {2: 0, 0: 5, 1: 0}),\n",
       "             14: defaultdict(int, {0: 0, 2: 3, 1: 0})})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTopicMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 토픽\n",
      "[('r', 4), ('statistics', 3), ('probability', 3), ('python', 3)]\n",
      "0 번째 토픽\n",
      "[('hbase', 3), ('postgres', 2), ('big data', 2), ('hadoop', 2)]\n",
      "2 번째 토픽\n",
      "[('regression', 2), ('neural networks', 2), ('artificial intelligence', 2), ('deep learning', 2)]\n"
     ]
    }
   ],
   "source": [
    "for k, termList in topicTermMatrix.items():\n",
    "    print(k, \"번째 토픽\")\n",
    "    print(sorted(termList.items(), key=lambda x:x[1], reverse=True)[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 내 수집 데이터(국제, 생활) => 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def fileids(path):\n",
    "    return [path+file for file in listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filecontent(file):\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def makePattern():\n",
    "    pattern = dict()\n",
    "\n",
    "    # 구두점\n",
    "    pattern1 = re.compile(r'[{0}]'.format(re.escape(punctuation)))\n",
    "    pattern['punc'] = pattern1\n",
    "    # corpus = pattern1.sub(' ',corpus)\n",
    "\n",
    "    # 불용어\n",
    "    pattern2 = re.compile(r'[A-Za-z0-9]{7,}')\n",
    "    pattern['stop'] = pattern2\n",
    "    # corpus = pattern2.sub(' ',corpus)\n",
    "\n",
    "    # 이메일\n",
    "    # pattern3 = re.compile(r'\\w{2,}@\\w{3,}(.\\w{2,})+')\n",
    "    pattern3 = re.compile(r'\\w{2,}@(.?\\w{2,})+')\n",
    "    pattern['email'] = pattern3\n",
    "    # corpus = pattern3.sub(' ',corpus)\n",
    "\n",
    "    # 도메인\n",
    "    pattern4 = re.compile(r'(.?\\w{2,}){2,}')\n",
    "    pattern['url'] = pattern4\n",
    "    # corpus = pattern4.sub(' ',corpus)\n",
    "\n",
    "    # 한글 이외\n",
    "    pattern5 = re.compile(r'[^가-힣0-9]+')\n",
    "    pattern['nonkorean'] = pattern5\n",
    "    # corpus = pattern5.sub(' ',corpus)\n",
    "\n",
    "    # WhiteSpace\n",
    "    pattern6 = re.compile(r\"\\s{2,}\")\n",
    "    pattern['whitespace'] = pattern5\n",
    "    # corpus = pattern6.sub(' ',corpus)\n",
    "    \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Komoran\n",
    "# content = filecontent(fileids('./news_crawl_project/')[-2])\n",
    "ma = Komoran()\n",
    "pattern = makePattern()\n",
    "\n",
    "def punc_stop(file):\n",
    "    for _ in ['email', 'punc', 'stop','whitespace']:\n",
    "        file = pattern[_].sub(' ',file)\n",
    "    return file\n",
    "\n",
    "def indexing(file):\n",
    "    indexTerm1 = defaultdict(int)\n",
    "    indexTerm2 = defaultdict(int)\n",
    "    indexTerm3 = defaultdict(int)\n",
    "    indexTerm4 = defaultdict(int)\n",
    "    for term in word_tokenize(file):\n",
    "        indexTerm1[term] += 1 # 원시어절\n",
    "    \n",
    "    for _ in indexTerm1:\n",
    "        for t in ma.pos(_):\n",
    "            \n",
    "            indexTerm2[t] += 1 # 원시형태소+품사\n",
    "            if len(t[0]) > 1: \n",
    "                if t[1].startswith('N'):\n",
    "                    indexTerm4[t[0]] += 1 # 명사\n",
    "    \n",
    "    return indexTerm4 #일단은 명사만 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 진행중\n",
      "200 진행중\n",
      "300 진행중\n",
      "400 진행중\n",
      "500 진행중\n",
      "600 진행중\n",
      "700 진행중\n"
     ]
    }
   ],
   "source": [
    "documentList = defaultdict(lambda: defaultdict(int))\n",
    "idx = 0\n",
    "for file in fileids('./news_crawl_project/'):\n",
    "    if file.split('/')[-1][:2] in [\"정치\", \"경제\", \"사회\"]:\n",
    "        documentList[idx] = indexing(punc_stop(filecontent(file)))\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx, '진행중')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['앵커',\n",
       " '삼성그룹',\n",
       " '노조',\n",
       " '파괴',\n",
       " '의혹',\n",
       " '검찰',\n",
       " '지금',\n",
       " '합병',\n",
       " '에버랜드',\n",
       " '수사',\n",
       " '결과',\n",
       " '그룹',\n",
       " '지휘부',\n",
       " '역할',\n",
       " '미래',\n",
       " '전략',\n",
       " '계열사',\n",
       " '노무',\n",
       " '담당자',\n",
       " '조직',\n",
       " '개입',\n",
       " '판단',\n",
       " '저희',\n",
       " '노사',\n",
       " '문건',\n",
       " '보도',\n",
       " '와해',\n",
       " '제기',\n",
       " '실체',\n",
       " '이번',\n",
       " '오너',\n",
       " '일가',\n",
       " '여부',\n",
       " '주목',\n",
       " '김선미',\n",
       " '기자',\n",
       " '삼성',\n",
       " '설립',\n",
       " '공작',\n",
       " '당시',\n",
       " '간부',\n",
       " '미행',\n",
       " '술자리',\n",
       " '확인',\n",
       " '경찰',\n",
       " '음주운전',\n",
       " '혐의',\n",
       " '신고',\n",
       " '알코올',\n",
       " '농도',\n",
       " '수준',\n",
       " '체포',\n",
       " '실패',\n",
       " '사찰',\n",
       " '포차',\n",
       " '운행',\n",
       " '사실',\n",
       " '이후',\n",
       " '차량',\n",
       " '번호',\n",
       " '촬영',\n",
       " '의뢰',\n",
       " '해당',\n",
       " '해고',\n",
       " '작성',\n",
       " '적시',\n",
       " '조장희',\n",
       " '부위원장',\n",
       " '문제',\n",
       " '인력',\n",
       " '주동자',\n",
       " '컨트롤',\n",
       " '타워',\n",
       " '주도',\n",
       " '동참',\n",
       " '전인',\n",
       " '직원',\n",
       " '동원',\n",
       " '어용',\n",
       " '신고서',\n",
       " '서류',\n",
       " '대신',\n",
       " '언론',\n",
       " '대응',\n",
       " '방법',\n",
       " '복수',\n",
       " '회사',\n",
       " '교섭',\n",
       " '창구',\n",
       " '악용',\n",
       " '임금',\n",
       " '협약',\n",
       " '오늘',\n",
       " '강경',\n",
       " '삼성전자',\n",
       " '부사장',\n",
       " '이모',\n",
       " '전무',\n",
       " '재판',\n",
       " '영상',\n",
       " '디자인',\n",
       " '이정',\n",
       " '취재',\n",
       " '후원',\n",
       " '편집',\n",
       " '경화',\n",
       " '클릭',\n",
       " '정신',\n",
       " '환자',\n",
       " '상담',\n",
       " '흉기',\n",
       " '의사',\n",
       " '사망',\n",
       " '운영위',\n",
       " '박범계',\n",
       " '의원',\n",
       " '한마디',\n",
       " '축구',\n",
       " '천재',\n",
       " '사비',\n",
       " '망언',\n",
       " '한국',\n",
       " '아시안컵',\n",
       " '탈락',\n",
       " '12월',\n",
       " '뉴스룸',\n",
       " '한번',\n",
       " '이어',\n",
       " '보기',\n",
       " '비트',\n",
       " '레전드',\n",
       " '콘텐트',\n",
       " '기사',\n",
       " '저작권법',\n",
       " '보호',\n",
       " '무단',\n",
       " '전재',\n",
       " '복사',\n",
       " '배포']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(documentList[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.1\n",
    "b = 0.1\n",
    "K = 3\n",
    "\n",
    "docTermTopicMat = list()\n",
    "Vocaburary = list()\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "for d in documentList:\n",
    "    termTopic = list()\n",
    "    for t in list(documentList[d]):\n",
    "        termTopic.append([t.lower(), random.randrange(K)])\n",
    "        Vocaburary.append(t.lower())\n",
    "    docTermTopicMat.append(termTopic)\n",
    "Vocaburary = list(set(Vocaburary))\n",
    "        \n",
    "M = len(docTermTopicMat)\n",
    "N = len(Vocaburary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2b0769c8f54c3caa1cc433c27fb7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "_iter = 5\n",
    "bar_total = tqdm_notebook(range(_iter))\n",
    "\n",
    "for _ in bar_total:\n",
    "    for i, termTopic in enumerate(docTermTopicMat):\n",
    "        for row in termTopic:\n",
    "            topicTermMatrix[row[1]][row[0]] -= 1\n",
    "            docTopicMatrix[i][row[1]] -= 1\n",
    "            \n",
    "            k = topicAssign(i, row[0])\n",
    "            \n",
    "            row[1] = k #토픽 새로 assign\n",
    "            topicTermMatrix[row[1]][row[0]] += 1\n",
    "            docTopicMatrix[i][row[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, termList in topicTermMatrix.items():\n",
    "    print(k, \"번째 토픽\")\n",
    "    print(sorted(termList.items(), key=lambda x:x[1], reverse=True)[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
